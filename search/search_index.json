{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"flame-pytorch","text":""},{"location":"reference/liblaf/flame_pytorch/","title":"flame_pytorch","text":""},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch","title":"liblaf.flame_pytorch","text":"<p>Modules:</p> <ul> <li> <code>config</code>           \u2013            </li> <li> <code>flame</code>           \u2013            </li> <li> <code>upstream</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>FLAME</code>           \u2013            </li> <li> <code>FlameConfig</code>           \u2013            </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>               (<code>str</code>)           \u2013            </li> <li> <code>__version_tuple__</code>               (<code>tuple[int | str, ...]</code>)           \u2013            </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__: str = '0.1.dev13+gae0e0ddbe'\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.__version_tuple__","title":"__version_tuple__  <code>module-attribute</code>","text":"<pre><code>__version_tuple__: tuple[int | str, ...] = (\n    0,\n    1,\n    \"dev13\",\n    \"gae0e0ddbe\",\n)\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME","title":"FLAME","text":"<pre><code>FLAME(config: FlameConfig | None = None)\n</code></pre> <p>               Bases: <code>FLAME</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              </li> <li> <code>forward</code>             \u2013              <p>Input:</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>NECK_IDX</code>           \u2013            </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>config</code>               (<code>FlameConfig</code>)           \u2013            </li> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            </li> <li> <code>faces</code>               (<code>Integer[ndarray, 'faces 3']</code>)           \u2013            </li> <li> <code>flame_model</code>           \u2013            </li> <li> <code>shapedirs</code>               (<code>Tensor</code>)           \u2013            </li> <li> <code>use_3D_translation</code>               (<code>bool</code>)           \u2013            </li> <li> <code>use_face_contour</code>               (<code>bool</code>)           \u2013            </li> </ul> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>def __init__(self, config: FlameConfig | None = None) -&gt; None:\n    if config is None:\n        config = FlameConfig()\n    super().__init__(config)\n    self.config = config\n    if torch.cuda.is_available():\n        self.cuda()\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.NECK_IDX","title":"NECK_IDX  <code>instance-attribute</code>","text":"<pre><code>NECK_IDX = 1\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: int\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: FlameConfig = config\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype: dtype\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.faces","title":"faces  <code>instance-attribute</code>","text":"<pre><code>faces: Integer[ndarray, 'faces 3']\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.flame_model","title":"flame_model  <code>instance-attribute</code>","text":"<pre><code>flame_model = Struct(**(load(f, encoding='latin1')))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.shapedirs","title":"shapedirs  <code>instance-attribute</code>","text":"<pre><code>shapedirs: Tensor\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.use_3D_translation","title":"use_3D_translation  <code>instance-attribute</code>","text":"<pre><code>use_3D_translation: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.use_face_contour","title":"use_face_contour  <code>instance-attribute</code>","text":"<pre><code>use_face_contour: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.__call__","title":"__call__","text":"<pre><code>__call__(\n    shape: Float[Tensor, \"batch shape\"] | None = None,\n    expression: Float[Tensor, \"batch expression\"]\n    | None = None,\n    pose: Float[Tensor, \"batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"batch 6\"] | None = None,\n    translation: Float[Tensor, \"batch 3\"] | None = None,\n) -&gt; tuple[\n    Float[Tensor, \"batch vertices\"],\n    Float[Tensor, \"batch landmarks 3\"],\n]\n</code></pre> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>    self.config = config\n    if torch.cuda.is_available():\n        self.cuda()\n\ndef forward(  # pyright: ignore[reportIncompatibleMethodOverride]\n    self,\n    shape: Float[Tensor, \"#batch shape\"] | None = None,\n    expression: Float[Tensor, \"#batch expression\"] | None = None,\n    pose: Float[Tensor, \"#batch pose\"] | None = None,\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FLAME.forward","title":"forward","text":"<pre><code>forward(\n    shape: Float[Tensor, \"batch shape\"] | None = None,\n    expression: Float[Tensor, \"batch expression\"]\n    | None = None,\n    pose: Float[Tensor, \"batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"batch 6\"] | None = None,\n    translation: Float[Tensor, \"batch 3\"] | None = None,\n) -&gt; tuple[\n    Float[Tensor, \"batch vertices\"],\n    Float[Tensor, \"batch landmarks 3\"],\n]\n</code></pre> Input <p>shape_params: N X number of shape parameters expression_params: N X number of expression parameters pose_params: N X number of pose parameters</p> <p>return:     vertices: N X V X 3     landmarks: N X number of landmarks X 3</p> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>def forward(  # pyright: ignore[reportIncompatibleMethodOverride]\n    self,\n    shape: Float[Tensor, \"#batch shape\"] | None = None,\n    expression: Float[Tensor, \"#batch expression\"] | None = None,\n    pose: Float[Tensor, \"#batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"#batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"#batch 6\"] | None = None,\n    translation: Float[Tensor, \"#batch 3\"] | None = None,\n) -&gt; tuple[Float[Tensor, \"#batch vertices 3\"], Float[Tensor, \"#batch landmarks 3\"]]:\n    if shape is None:\n        shape = torch.zeros(\n            (self.config.batch_size, self.config.shape_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    if expression is None:\n        expression = torch.zeros(\n            (self.config.batch_size, self.config.expression_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    if pose is None:\n        pose = torch.zeros(\n            (self.config.batch_size, self.config.pose_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    return super().forward(\n        shape_params=shape,\n        expression_params=expression,\n        pose_params=pose,\n        neck_pose=neck_pose,\n        eye_pose=eye_pose,\n        transl=translation,\n    )\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig","title":"FlameConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters:</p> <ul> <li> <code>flame_model_path</code>               (<code>Path</code>, default:                   <code>PosixPath('model/generic_model.pkl')</code> )           \u2013            <p>flame model path</p> </li> <li> <code>static_landmark_embedding_path</code>               (<code>Path</code>, default:                   <code>PosixPath('/home/runner/.cache/liblaf/flame-pytorch/flame_static_embedding.pkl')</code> )           \u2013            <p>Static landmark embeddings path for FLAME</p> </li> <li> <code>dynamic_landmark_embedding_path</code>               (<code>Path</code>, default:                   <code>PosixPath('/home/runner/.cache/liblaf/flame-pytorch/flame_dynamic_embedding.npy')</code> )           \u2013            <p>Dynamic contour embedding path for FLAME</p> </li> <li> <code>shape_params</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>the number of shape parameters</p> </li> <li> <code>expression_params</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>the number of expression parameters</p> </li> <li> <code>pose_params</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>the number of pose parameters</p> </li> <li> <code>use_face_contour</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true apply the landmark loss on also on the face contour.</p> </li> <li> <code>use_3d_translation</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true apply the landmark loss on also on the face contour.</p> </li> <li> <code>optimize_eyeballpose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true optimize for the eyeball pose.</p> </li> <li> <code>optimize_neckpose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true optimize for the neck pose.</p> </li> <li> <code>num_worker</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>pytorch number worker.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Training batch size.</p> </li> <li> <code>ring_margin</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>ring margin.</p> </li> <li> <code>ring_loss_weight</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>weight on ring loss.</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"properties\": {\n    \"flame_model_path\": {\n      \"format\": \"path\",\n      \"title\": \"Flame Model Path\",\n      \"type\": \"string\"\n    },\n    \"static_landmark_embedding_path\": {\n      \"format\": \"path\",\n      \"title\": \"Static Landmark Embedding Path\",\n      \"type\": \"string\"\n    },\n    \"dynamic_landmark_embedding_path\": {\n      \"format\": \"path\",\n      \"title\": \"Dynamic Landmark Embedding Path\",\n      \"type\": \"string\"\n    },\n    \"shape_params\": {\n      \"default\": 100,\n      \"title\": \"Shape Params\",\n      \"type\": \"integer\"\n    },\n    \"expression_params\": {\n      \"default\": 50,\n      \"title\": \"Expression Params\",\n      \"type\": \"integer\"\n    },\n    \"pose_params\": {\n      \"default\": 6,\n      \"title\": \"Pose Params\",\n      \"type\": \"integer\"\n    },\n    \"use_face_contour\": {\n      \"default\": true,\n      \"title\": \"Use Face Contour\",\n      \"type\": \"boolean\"\n    },\n    \"use_3d_translation\": {\n      \"default\": true,\n      \"title\": \"Use 3D Translation\",\n      \"type\": \"boolean\"\n    },\n    \"optimize_eyeballpose\": {\n      \"default\": true,\n      \"title\": \"Optimize Eyeballpose\",\n      \"type\": \"boolean\"\n    },\n    \"optimize_neckpose\": {\n      \"default\": true,\n      \"title\": \"Optimize Neckpose\",\n      \"type\": \"boolean\"\n    },\n    \"num_worker\": {\n      \"default\": 4,\n      \"title\": \"Num Worker\",\n      \"type\": \"integer\"\n    },\n    \"batch_size\": {\n      \"default\": 1,\n      \"title\": \"Batch Size\",\n      \"type\": \"integer\"\n    },\n    \"ring_margin\": {\n      \"default\": 0.5,\n      \"title\": \"Ring Margin\",\n      \"type\": \"number\"\n    },\n    \"ring_loss_weight\": {\n      \"default\": 1.0,\n      \"title\": \"Ring Loss Weight\",\n      \"type\": \"number\"\n    }\n  },\n  \"title\": \"FlameConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flame_model_path</code>                 (<code>Path</code>)             </li> <li> <code>static_landmark_embedding_path</code>                 (<code>Path</code>)             </li> <li> <code>dynamic_landmark_embedding_path</code>                 (<code>Path</code>)             </li> <li> <code>shape_params</code>                 (<code>int</code>)             </li> <li> <code>expression_params</code>                 (<code>int</code>)             </li> <li> <code>pose_params</code>                 (<code>int</code>)             </li> <li> <code>use_face_contour</code>                 (<code>bool</code>)             </li> <li> <code>use_3d_translation</code>                 (<code>bool</code>)             </li> <li> <code>optimize_eyeballpose</code>                 (<code>bool</code>)             </li> <li> <code>optimize_neckpose</code>                 (<code>bool</code>)             </li> <li> <code>num_worker</code>                 (<code>int</code>)             </li> <li> <code>batch_size</code>                 (<code>int</code>)             </li> <li> <code>ring_margin</code>                 (<code>float</code>)             </li> <li> <code>ring_loss_weight</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.batch_size","title":"batch_size  <code>pydantic-field</code>","text":"<pre><code>batch_size: int = 1\n</code></pre> <p>Training batch size.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.dynamic_landmark_embedding_path","title":"dynamic_landmark_embedding_path  <code>pydantic-field</code>","text":"<pre><code>dynamic_landmark_embedding_path: Path\n</code></pre> <p>Dynamic contour embedding path for FLAME</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.expression_params","title":"expression_params  <code>pydantic-field</code>","text":"<pre><code>expression_params: int = 50\n</code></pre> <p>the number of expression parameters</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.flame_model_path","title":"flame_model_path  <code>pydantic-field</code>","text":"<pre><code>flame_model_path: Path\n</code></pre> <p>flame model path</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.num_worker","title":"num_worker  <code>pydantic-field</code>","text":"<pre><code>num_worker: int = 4\n</code></pre> <p>pytorch number worker.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.optimize_eyeballpose","title":"optimize_eyeballpose  <code>pydantic-field</code>","text":"<pre><code>optimize_eyeballpose: bool = True\n</code></pre> <p>If true optimize for the eyeball pose.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.optimize_neckpose","title":"optimize_neckpose  <code>pydantic-field</code>","text":"<pre><code>optimize_neckpose: bool = True\n</code></pre> <p>If true optimize for the neck pose.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.pose_params","title":"pose_params  <code>pydantic-field</code>","text":"<pre><code>pose_params: int = 6\n</code></pre> <p>the number of pose parameters</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.ring_loss_weight","title":"ring_loss_weight  <code>pydantic-field</code>","text":"<pre><code>ring_loss_weight: float = 1.0\n</code></pre> <p>weight on ring loss.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.ring_margin","title":"ring_margin  <code>pydantic-field</code>","text":"<pre><code>ring_margin: float = 0.5\n</code></pre> <p>ring margin.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.shape_params","title":"shape_params  <code>pydantic-field</code>","text":"<pre><code>shape_params: int = 100\n</code></pre> <p>the number of shape parameters</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.static_landmark_embedding_path","title":"static_landmark_embedding_path  <code>pydantic-field</code>","text":"<pre><code>static_landmark_embedding_path: Path\n</code></pre> <p>Static landmark embeddings path for FLAME</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.use_3D_translation","title":"use_3D_translation  <code>property</code> <code>writable</code>","text":"<pre><code>use_3D_translation: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.use_3d_translation","title":"use_3d_translation  <code>pydantic-field</code>","text":"<pre><code>use_3d_translation: bool = True\n</code></pre> <p>If true apply the landmark loss on also on the face contour.</p>"},{"location":"reference/liblaf/flame_pytorch/#liblaf.flame_pytorch.FlameConfig.use_face_contour","title":"use_face_contour  <code>pydantic-field</code>","text":"<pre><code>use_face_contour: bool = True\n</code></pre> <p>If true apply the landmark loss on also on the face contour.</p>"},{"location":"reference/liblaf/flame_pytorch/config/","title":"config","text":""},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config","title":"liblaf.flame_pytorch.config","text":"<p>Classes:</p> <ul> <li> <code>FlameConfig</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>get_config</code>             \u2013              </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig","title":"FlameConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters:</p> <ul> <li> <code>flame_model_path</code>               (<code>Path</code>, default:                   <code>PosixPath('model/generic_model.pkl')</code> )           \u2013            <p>flame model path</p> </li> <li> <code>static_landmark_embedding_path</code>               (<code>Path</code>, default:                   <code>PosixPath('/home/runner/.cache/liblaf/flame-pytorch/flame_static_embedding.pkl')</code> )           \u2013            <p>Static landmark embeddings path for FLAME</p> </li> <li> <code>dynamic_landmark_embedding_path</code>               (<code>Path</code>, default:                   <code>PosixPath('/home/runner/.cache/liblaf/flame-pytorch/flame_dynamic_embedding.npy')</code> )           \u2013            <p>Dynamic contour embedding path for FLAME</p> </li> <li> <code>shape_params</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>the number of shape parameters</p> </li> <li> <code>expression_params</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>the number of expression parameters</p> </li> <li> <code>pose_params</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>the number of pose parameters</p> </li> <li> <code>use_face_contour</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true apply the landmark loss on also on the face contour.</p> </li> <li> <code>use_3d_translation</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true apply the landmark loss on also on the face contour.</p> </li> <li> <code>optimize_eyeballpose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true optimize for the eyeball pose.</p> </li> <li> <code>optimize_neckpose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true optimize for the neck pose.</p> </li> <li> <code>num_worker</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>pytorch number worker.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Training batch size.</p> </li> <li> <code>ring_margin</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>ring margin.</p> </li> <li> <code>ring_loss_weight</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>weight on ring loss.</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"properties\": {\n    \"flame_model_path\": {\n      \"format\": \"path\",\n      \"title\": \"Flame Model Path\",\n      \"type\": \"string\"\n    },\n    \"static_landmark_embedding_path\": {\n      \"format\": \"path\",\n      \"title\": \"Static Landmark Embedding Path\",\n      \"type\": \"string\"\n    },\n    \"dynamic_landmark_embedding_path\": {\n      \"format\": \"path\",\n      \"title\": \"Dynamic Landmark Embedding Path\",\n      \"type\": \"string\"\n    },\n    \"shape_params\": {\n      \"default\": 100,\n      \"title\": \"Shape Params\",\n      \"type\": \"integer\"\n    },\n    \"expression_params\": {\n      \"default\": 50,\n      \"title\": \"Expression Params\",\n      \"type\": \"integer\"\n    },\n    \"pose_params\": {\n      \"default\": 6,\n      \"title\": \"Pose Params\",\n      \"type\": \"integer\"\n    },\n    \"use_face_contour\": {\n      \"default\": true,\n      \"title\": \"Use Face Contour\",\n      \"type\": \"boolean\"\n    },\n    \"use_3d_translation\": {\n      \"default\": true,\n      \"title\": \"Use 3D Translation\",\n      \"type\": \"boolean\"\n    },\n    \"optimize_eyeballpose\": {\n      \"default\": true,\n      \"title\": \"Optimize Eyeballpose\",\n      \"type\": \"boolean\"\n    },\n    \"optimize_neckpose\": {\n      \"default\": true,\n      \"title\": \"Optimize Neckpose\",\n      \"type\": \"boolean\"\n    },\n    \"num_worker\": {\n      \"default\": 4,\n      \"title\": \"Num Worker\",\n      \"type\": \"integer\"\n    },\n    \"batch_size\": {\n      \"default\": 1,\n      \"title\": \"Batch Size\",\n      \"type\": \"integer\"\n    },\n    \"ring_margin\": {\n      \"default\": 0.5,\n      \"title\": \"Ring Margin\",\n      \"type\": \"number\"\n    },\n    \"ring_loss_weight\": {\n      \"default\": 1.0,\n      \"title\": \"Ring Loss Weight\",\n      \"type\": \"number\"\n    }\n  },\n  \"title\": \"FlameConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>flame_model_path</code>                 (<code>Path</code>)             </li> <li> <code>static_landmark_embedding_path</code>                 (<code>Path</code>)             </li> <li> <code>dynamic_landmark_embedding_path</code>                 (<code>Path</code>)             </li> <li> <code>shape_params</code>                 (<code>int</code>)             </li> <li> <code>expression_params</code>                 (<code>int</code>)             </li> <li> <code>pose_params</code>                 (<code>int</code>)             </li> <li> <code>use_face_contour</code>                 (<code>bool</code>)             </li> <li> <code>use_3d_translation</code>                 (<code>bool</code>)             </li> <li> <code>optimize_eyeballpose</code>                 (<code>bool</code>)             </li> <li> <code>optimize_neckpose</code>                 (<code>bool</code>)             </li> <li> <code>num_worker</code>                 (<code>int</code>)             </li> <li> <code>batch_size</code>                 (<code>int</code>)             </li> <li> <code>ring_margin</code>                 (<code>float</code>)             </li> <li> <code>ring_loss_weight</code>                 (<code>float</code>)             </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.batch_size","title":"batch_size  <code>pydantic-field</code>","text":"<pre><code>batch_size: int = 1\n</code></pre> <p>Training batch size.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.dynamic_landmark_embedding_path","title":"dynamic_landmark_embedding_path  <code>pydantic-field</code>","text":"<pre><code>dynamic_landmark_embedding_path: Path\n</code></pre> <p>Dynamic contour embedding path for FLAME</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.expression_params","title":"expression_params  <code>pydantic-field</code>","text":"<pre><code>expression_params: int = 50\n</code></pre> <p>the number of expression parameters</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.flame_model_path","title":"flame_model_path  <code>pydantic-field</code>","text":"<pre><code>flame_model_path: Path\n</code></pre> <p>flame model path</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.num_worker","title":"num_worker  <code>pydantic-field</code>","text":"<pre><code>num_worker: int = 4\n</code></pre> <p>pytorch number worker.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.optimize_eyeballpose","title":"optimize_eyeballpose  <code>pydantic-field</code>","text":"<pre><code>optimize_eyeballpose: bool = True\n</code></pre> <p>If true optimize for the eyeball pose.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.optimize_neckpose","title":"optimize_neckpose  <code>pydantic-field</code>","text":"<pre><code>optimize_neckpose: bool = True\n</code></pre> <p>If true optimize for the neck pose.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.pose_params","title":"pose_params  <code>pydantic-field</code>","text":"<pre><code>pose_params: int = 6\n</code></pre> <p>the number of pose parameters</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.ring_loss_weight","title":"ring_loss_weight  <code>pydantic-field</code>","text":"<pre><code>ring_loss_weight: float = 1.0\n</code></pre> <p>weight on ring loss.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.ring_margin","title":"ring_margin  <code>pydantic-field</code>","text":"<pre><code>ring_margin: float = 0.5\n</code></pre> <p>ring margin.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.shape_params","title":"shape_params  <code>pydantic-field</code>","text":"<pre><code>shape_params: int = 100\n</code></pre> <p>the number of shape parameters</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.static_landmark_embedding_path","title":"static_landmark_embedding_path  <code>pydantic-field</code>","text":"<pre><code>static_landmark_embedding_path: Path\n</code></pre> <p>Static landmark embeddings path for FLAME</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.use_3D_translation","title":"use_3D_translation  <code>property</code> <code>writable</code>","text":"<pre><code>use_3D_translation: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.use_3d_translation","title":"use_3d_translation  <code>pydantic-field</code>","text":"<pre><code>use_3d_translation: bool = True\n</code></pre> <p>If true apply the landmark loss on also on the face contour.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.FlameConfig.use_face_contour","title":"use_face_contour  <code>pydantic-field</code>","text":"<pre><code>use_face_contour: bool = True\n</code></pre> <p>If true apply the landmark loss on also on the face contour.</p>"},{"location":"reference/liblaf/flame_pytorch/config/#liblaf.flame_pytorch.config.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; FlameConfig\n</code></pre> Source code in <code>src/liblaf/flame_pytorch/config.py</code> <pre><code>def get_config() -&gt; FlameConfig:\n    return FlameConfig()\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/","title":"flame","text":""},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame","title":"liblaf.flame_pytorch.flame","text":"<p>Classes:</p> <ul> <li> <code>FLAME</code>           \u2013            </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME","title":"FLAME","text":"<pre><code>FLAME(config: FlameConfig | None = None)\n</code></pre> <p>               Bases: <code>FLAME</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              </li> <li> <code>forward</code>             \u2013              <p>Input:</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>NECK_IDX</code>           \u2013            </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>config</code>               (<code>FlameConfig</code>)           \u2013            </li> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            </li> <li> <code>faces</code>               (<code>Integer[ndarray, 'faces 3']</code>)           \u2013            </li> <li> <code>flame_model</code>           \u2013            </li> <li> <code>shapedirs</code>               (<code>Tensor</code>)           \u2013            </li> <li> <code>use_3D_translation</code>               (<code>bool</code>)           \u2013            </li> <li> <code>use_face_contour</code>               (<code>bool</code>)           \u2013            </li> </ul> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>def __init__(self, config: FlameConfig | None = None) -&gt; None:\n    if config is None:\n        config = FlameConfig()\n    super().__init__(config)\n    self.config = config\n    if torch.cuda.is_available():\n        self.cuda()\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.NECK_IDX","title":"NECK_IDX  <code>instance-attribute</code>","text":"<pre><code>NECK_IDX = 1\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: int\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: FlameConfig = config\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype: dtype\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.faces","title":"faces  <code>instance-attribute</code>","text":"<pre><code>faces: Integer[ndarray, 'faces 3']\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.flame_model","title":"flame_model  <code>instance-attribute</code>","text":"<pre><code>flame_model = Struct(**(load(f, encoding='latin1')))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.shapedirs","title":"shapedirs  <code>instance-attribute</code>","text":"<pre><code>shapedirs: Tensor\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.use_3D_translation","title":"use_3D_translation  <code>instance-attribute</code>","text":"<pre><code>use_3D_translation: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.use_face_contour","title":"use_face_contour  <code>instance-attribute</code>","text":"<pre><code>use_face_contour: bool\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.__call__","title":"__call__","text":"<pre><code>__call__(\n    shape: Float[Tensor, \"batch shape\"] | None = None,\n    expression: Float[Tensor, \"batch expression\"]\n    | None = None,\n    pose: Float[Tensor, \"batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"batch 6\"] | None = None,\n    translation: Float[Tensor, \"batch 3\"] | None = None,\n) -&gt; tuple[\n    Float[Tensor, \"batch vertices\"],\n    Float[Tensor, \"batch landmarks 3\"],\n]\n</code></pre> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>    self.config = config\n    if torch.cuda.is_available():\n        self.cuda()\n\ndef forward(  # pyright: ignore[reportIncompatibleMethodOverride]\n    self,\n    shape: Float[Tensor, \"#batch shape\"] | None = None,\n    expression: Float[Tensor, \"#batch expression\"] | None = None,\n    pose: Float[Tensor, \"#batch pose\"] | None = None,\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/flame/#liblaf.flame_pytorch.flame.FLAME.forward","title":"forward","text":"<pre><code>forward(\n    shape: Float[Tensor, \"batch shape\"] | None = None,\n    expression: Float[Tensor, \"batch expression\"]\n    | None = None,\n    pose: Float[Tensor, \"batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"batch 6\"] | None = None,\n    translation: Float[Tensor, \"batch 3\"] | None = None,\n) -&gt; tuple[\n    Float[Tensor, \"batch vertices\"],\n    Float[Tensor, \"batch landmarks 3\"],\n]\n</code></pre> Input <p>shape_params: N X number of shape parameters expression_params: N X number of expression parameters pose_params: N X number of pose parameters</p> <p>return:     vertices: N X V X 3     landmarks: N X number of landmarks X 3</p> Source code in <code>src/liblaf/flame_pytorch/flame.py</code> <pre><code>def forward(  # pyright: ignore[reportIncompatibleMethodOverride]\n    self,\n    shape: Float[Tensor, \"#batch shape\"] | None = None,\n    expression: Float[Tensor, \"#batch expression\"] | None = None,\n    pose: Float[Tensor, \"#batch pose\"] | None = None,\n    neck_pose: Float[Tensor, \"#batch 3\"] | None = None,\n    eye_pose: Float[Tensor, \"#batch 6\"] | None = None,\n    translation: Float[Tensor, \"#batch 3\"] | None = None,\n) -&gt; tuple[Float[Tensor, \"#batch vertices 3\"], Float[Tensor, \"#batch landmarks 3\"]]:\n    if shape is None:\n        shape = torch.zeros(\n            (self.config.batch_size, self.config.shape_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    if expression is None:\n        expression = torch.zeros(\n            (self.config.batch_size, self.config.expression_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    if pose is None:\n        pose = torch.zeros(\n            (self.config.batch_size, self.config.pose_params),\n            device=self.shapedirs.device,\n            requires_grad=False,\n        )\n    return super().forward(\n        shape_params=shape,\n        expression_params=expression,\n        pose_params=pose,\n        neck_pose=neck_pose,\n        eye_pose=eye_pose,\n        transl=translation,\n    )\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/","title":"upstream","text":""},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream","title":"liblaf.flame_pytorch.upstream","text":"<p>Modules:</p> <ul> <li> <code>config</code>           \u2013            </li> <li> <code>flame</code>           \u2013            <p>FLAME Layer: Implementation of the 3D Statistical Face model in PyTorch</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>FLAME</code>           \u2013            <p>Given flame parameters this class generates a differentiable FLAME function</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_config</code>             \u2013              </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME","title":"FLAME","text":"<pre><code>FLAME(config)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Given flame parameters this class generates a differentiable FLAME function which outputs the a mesh and 3D facial landmarks</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Input:</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>NECK_IDX</code>           \u2013            </li> <li> <code>batch_size</code>           \u2013            </li> <li> <code>dtype</code>           \u2013            </li> <li> <code>faces</code>           \u2013            </li> <li> <code>flame_model</code>           \u2013            </li> <li> <code>use_3D_translation</code>           \u2013            </li> <li> <code>use_face_contour</code>           \u2013            </li> </ul> Source code in <code>src/liblaf/flame_pytorch/upstream/flame.py</code> <pre><code>def __init__(self, config):\n    super(FLAME, self).__init__()\n    print(\"creating the FLAME Decoder\")\n    with open(config.flame_model_path, \"rb\") as f:\n        self.flame_model = Struct(**pickle.load(f, encoding=\"latin1\"))\n    self.NECK_IDX = 1\n    self.batch_size = config.batch_size\n    self.dtype = torch.float32\n    self.use_face_contour = config.use_face_contour\n    self.faces = self.flame_model.f\n    self.register_buffer(\n        \"faces_tensor\",\n        to_tensor(to_np(self.faces, dtype=np.int64), dtype=torch.long),\n    )\n\n    # Fixing remaining Shape betas\n    # There are total 300 shape parameters to control FLAME; But one can use the first few parameters to express\n    # the shape. For example 100 shape parameters are used for RingNet project\n    default_shape = torch.zeros(\n        [self.batch_size, 300 - config.shape_params],\n        dtype=self.dtype,\n        requires_grad=False,\n    )\n    self.register_parameter(\n        \"shape_betas\", nn.Parameter(default_shape, requires_grad=False)\n    )\n\n    # Fixing remaining expression betas\n    # There are total 100 shape expression parameters to control FLAME; But one can use the first few parameters to express\n    # the expression. For example 50 expression parameters are used for RingNet project\n    default_exp = torch.zeros(\n        [self.batch_size, 100 - config.expression_params],\n        dtype=self.dtype,\n        requires_grad=False,\n    )\n    self.register_parameter(\n        \"expression_betas\", nn.Parameter(default_exp, requires_grad=False)\n    )\n\n    # Eyeball and neck rotation\n    default_eyball_pose = torch.zeros(\n        [self.batch_size, 6], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"eye_pose\", nn.Parameter(default_eyball_pose, requires_grad=False)\n    )\n\n    default_neck_pose = torch.zeros(\n        [self.batch_size, 3], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"neck_pose\", nn.Parameter(default_neck_pose, requires_grad=False)\n    )\n\n    # Fixing 3D translation since we use translation in the image plane\n\n    self.use_3D_translation = config.use_3D_translation\n\n    default_transl = torch.zeros(\n        [self.batch_size, 3], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"transl\", nn.Parameter(default_transl, requires_grad=False)\n    )\n\n    # The vertices of the template model\n    self.register_buffer(\n        \"v_template\",\n        to_tensor(to_np(self.flame_model.v_template), dtype=self.dtype),\n    )\n\n    # The shape components\n    shapedirs = self.flame_model.shapedirs\n    # The shape components\n    self.register_buffer(\"shapedirs\", to_tensor(to_np(shapedirs), dtype=self.dtype))\n\n    j_regressor = to_tensor(to_np(self.flame_model.J_regressor), dtype=self.dtype)\n    self.register_buffer(\"J_regressor\", j_regressor)\n\n    # Pose blend shape basis\n    num_pose_basis = self.flame_model.posedirs.shape[-1]\n    posedirs = np.reshape(self.flame_model.posedirs, [-1, num_pose_basis]).T\n    self.register_buffer(\"posedirs\", to_tensor(to_np(posedirs), dtype=self.dtype))\n\n    # indices of parents for each joints\n    parents = to_tensor(to_np(self.flame_model.kintree_table[0])).long()\n    parents[0] = -1\n    self.register_buffer(\"parents\", parents)\n\n    self.register_buffer(\n        \"lbs_weights\", to_tensor(to_np(self.flame_model.weights), dtype=self.dtype)\n    )\n\n    # Static and Dynamic Landmark embeddings for FLAME\n\n    with open(config.static_landmark_embedding_path, \"rb\") as f:\n        static_embeddings = Struct(**pickle.load(f, encoding=\"latin1\"))\n\n    lmk_faces_idx = (static_embeddings.lmk_face_idx).astype(np.int64)\n    self.register_buffer(\n        \"lmk_faces_idx\", torch.tensor(lmk_faces_idx, dtype=torch.long)\n    )\n    lmk_bary_coords = static_embeddings.lmk_b_coords\n    self.register_buffer(\n        \"lmk_bary_coords\", torch.tensor(lmk_bary_coords, dtype=self.dtype)\n    )\n\n    if self.use_face_contour:\n        conture_embeddings = np.load(\n            config.dynamic_landmark_embedding_path,\n            allow_pickle=True,\n            encoding=\"latin1\",\n        )\n        conture_embeddings = conture_embeddings[()]\n        dynamic_lmk_faces_idx = np.array(conture_embeddings[\"lmk_face_idx\"]).astype(\n            np.int64\n        )\n        dynamic_lmk_faces_idx = torch.tensor(\n            dynamic_lmk_faces_idx, dtype=torch.long\n        )\n        self.register_buffer(\"dynamic_lmk_faces_idx\", dynamic_lmk_faces_idx)\n\n        dynamic_lmk_bary_coords = conture_embeddings[\"lmk_b_coords\"]\n        dynamic_lmk_bary_coords = np.array(dynamic_lmk_bary_coords)\n        dynamic_lmk_bary_coords = torch.tensor(\n            dynamic_lmk_bary_coords, dtype=self.dtype\n        )\n        self.register_buffer(\"dynamic_lmk_bary_coords\", dynamic_lmk_bary_coords)\n\n        neck_kin_chain = []\n        curr_idx = torch.tensor(self.NECK_IDX, dtype=torch.long)\n        while curr_idx != -1:\n            neck_kin_chain.append(curr_idx)\n            curr_idx = self.parents[curr_idx]\n        self.register_buffer(\"neck_kin_chain\", torch.stack(neck_kin_chain))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.NECK_IDX","title":"NECK_IDX  <code>instance-attribute</code>","text":"<pre><code>NECK_IDX = 1\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float32\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.faces","title":"faces  <code>instance-attribute</code>","text":"<pre><code>faces = f\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.flame_model","title":"flame_model  <code>instance-attribute</code>","text":"<pre><code>flame_model = Struct(**(load(f, encoding='latin1')))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.use_3D_translation","title":"use_3D_translation  <code>instance-attribute</code>","text":"<pre><code>use_3D_translation = use_3D_translation\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.use_face_contour","title":"use_face_contour  <code>instance-attribute</code>","text":"<pre><code>use_face_contour = use_face_contour\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.FLAME.forward","title":"forward","text":"<pre><code>forward(\n    shape_params=None,\n    expression_params=None,\n    pose_params=None,\n    neck_pose=None,\n    eye_pose=None,\n    transl=None,\n)\n</code></pre> Input <p>shape_params: N X number of shape parameters expression_params: N X number of expression parameters pose_params: N X number of pose parameters</p> <p>return:     vertices: N X V X 3     landmarks: N X number of landmarks X 3</p> Source code in <code>src/liblaf/flame_pytorch/upstream/flame.py</code> <pre><code>def forward(\n    self,\n    shape_params=None,\n    expression_params=None,\n    pose_params=None,\n    neck_pose=None,\n    eye_pose=None,\n    transl=None,\n):\n    \"\"\"\n    Input:\n        shape_params: N X number of shape parameters\n        expression_params: N X number of expression parameters\n        pose_params: N X number of pose parameters\n    return:\n        vertices: N X V X 3\n        landmarks: N X number of landmarks X 3\n    \"\"\"\n    betas = torch.cat(\n        [shape_params, self.shape_betas, expression_params, self.expression_betas],\n        dim=1,\n    )\n    neck_pose = neck_pose if neck_pose is not None else self.neck_pose\n    eye_pose = eye_pose if eye_pose is not None else self.eye_pose\n    transl = transl if transl is not None else self.transl\n    full_pose = torch.cat(\n        [pose_params[:, :3], neck_pose, pose_params[:, 3:], eye_pose], dim=1\n    )\n    template_vertices = self.v_template.unsqueeze(0).repeat(self.batch_size, 1, 1)\n\n    vertices, _ = lbs(\n        betas,\n        full_pose,\n        template_vertices,\n        self.shapedirs,\n        self.posedirs,\n        self.J_regressor,\n        self.parents,\n        self.lbs_weights,\n    )\n\n    lmk_faces_idx = self.lmk_faces_idx.unsqueeze(dim=0).repeat(self.batch_size, 1)\n    lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(\n        self.batch_size, 1, 1\n    )\n    if self.use_face_contour:\n\n        (\n            dyn_lmk_faces_idx,\n            dyn_lmk_bary_coords,\n        ) = self._find_dynamic_lmk_idx_and_bcoords(\n            vertices,\n            full_pose,\n            self.dynamic_lmk_faces_idx,\n            self.dynamic_lmk_bary_coords,\n            self.neck_kin_chain,\n            dtype=self.dtype,\n        )\n\n        lmk_faces_idx = torch.cat([dyn_lmk_faces_idx, lmk_faces_idx], 1)\n        lmk_bary_coords = torch.cat([dyn_lmk_bary_coords, lmk_bary_coords], 1)\n\n    landmarks = vertices2landmarks(\n        vertices, self.faces_tensor, lmk_faces_idx, lmk_bary_coords\n    )\n\n    if self.use_3D_translation:\n        landmarks += transl.unsqueeze(dim=1)\n        vertices += transl.unsqueeze(dim=1)\n\n    return vertices, landmarks\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/#liblaf.flame_pytorch.upstream.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> Source code in <code>src/liblaf/flame_pytorch/upstream/config.py</code> <pre><code>def get_config():\n    config = parser.parse_args()\n    return config\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/config/","title":"config","text":""},{"location":"reference/liblaf/flame_pytorch/upstream/config/#liblaf.flame_pytorch.upstream.config","title":"liblaf.flame_pytorch.upstream.config","text":"<p>Functions:</p> <ul> <li> <code>get_config</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>parser</code>           \u2013            </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/upstream/config/#liblaf.flame_pytorch.upstream.config.parser","title":"parser  <code>module-attribute</code>","text":"<pre><code>parser = ArgumentParser(description='FLAME model')\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/config/#liblaf.flame_pytorch.upstream.config.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> Source code in <code>src/liblaf/flame_pytorch/upstream/config.py</code> <pre><code>def get_config():\n    config = parser.parse_args()\n    return config\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/","title":"flame","text":""},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame","title":"liblaf.flame_pytorch.upstream.flame","text":"<p>FLAME Layer: Implementation of the 3D Statistical Face model in PyTorch</p> <p>It is designed in a way to directly plug in as a decoder layer in a Deep learning framework for training and testing</p> <p>It can also be used for 2D or 3D optimisation applications</p> <p>Author: Soubhik Sanyal Copyright \u00a9 2019, Soubhik Sanyal All rights reserved.</p> <p>Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this computer program. You can only use this computer program if you have closed a license agreement with MPG or you get the right to use the computer program from someone who is authorized to grant you that right. Any use of the computer program without a valid license is prohibited and liable to prosecution. Copyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics. All rights reserved.</p> <p>More information about FLAME is available at http://flame.is.tue.mpg.de.</p> <p>For questions regarding the PyTorch implementation please contact soubhik.sanyal@tuebingen.mpg.de</p> <p>Classes:</p> <ul> <li> <code>FLAME</code>           \u2013            <p>Given flame parameters this class generates a differentiable FLAME function</p> </li> </ul>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME","title":"FLAME","text":"<pre><code>FLAME(config)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Given flame parameters this class generates a differentiable FLAME function which outputs the a mesh and 3D facial landmarks</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Input:</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>NECK_IDX</code>           \u2013            </li> <li> <code>batch_size</code>           \u2013            </li> <li> <code>dtype</code>           \u2013            </li> <li> <code>faces</code>           \u2013            </li> <li> <code>flame_model</code>           \u2013            </li> <li> <code>use_3D_translation</code>           \u2013            </li> <li> <code>use_face_contour</code>           \u2013            </li> </ul> Source code in <code>src/liblaf/flame_pytorch/upstream/flame.py</code> <pre><code>def __init__(self, config):\n    super(FLAME, self).__init__()\n    print(\"creating the FLAME Decoder\")\n    with open(config.flame_model_path, \"rb\") as f:\n        self.flame_model = Struct(**pickle.load(f, encoding=\"latin1\"))\n    self.NECK_IDX = 1\n    self.batch_size = config.batch_size\n    self.dtype = torch.float32\n    self.use_face_contour = config.use_face_contour\n    self.faces = self.flame_model.f\n    self.register_buffer(\n        \"faces_tensor\",\n        to_tensor(to_np(self.faces, dtype=np.int64), dtype=torch.long),\n    )\n\n    # Fixing remaining Shape betas\n    # There are total 300 shape parameters to control FLAME; But one can use the first few parameters to express\n    # the shape. For example 100 shape parameters are used for RingNet project\n    default_shape = torch.zeros(\n        [self.batch_size, 300 - config.shape_params],\n        dtype=self.dtype,\n        requires_grad=False,\n    )\n    self.register_parameter(\n        \"shape_betas\", nn.Parameter(default_shape, requires_grad=False)\n    )\n\n    # Fixing remaining expression betas\n    # There are total 100 shape expression parameters to control FLAME; But one can use the first few parameters to express\n    # the expression. For example 50 expression parameters are used for RingNet project\n    default_exp = torch.zeros(\n        [self.batch_size, 100 - config.expression_params],\n        dtype=self.dtype,\n        requires_grad=False,\n    )\n    self.register_parameter(\n        \"expression_betas\", nn.Parameter(default_exp, requires_grad=False)\n    )\n\n    # Eyeball and neck rotation\n    default_eyball_pose = torch.zeros(\n        [self.batch_size, 6], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"eye_pose\", nn.Parameter(default_eyball_pose, requires_grad=False)\n    )\n\n    default_neck_pose = torch.zeros(\n        [self.batch_size, 3], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"neck_pose\", nn.Parameter(default_neck_pose, requires_grad=False)\n    )\n\n    # Fixing 3D translation since we use translation in the image plane\n\n    self.use_3D_translation = config.use_3D_translation\n\n    default_transl = torch.zeros(\n        [self.batch_size, 3], dtype=self.dtype, requires_grad=False\n    )\n    self.register_parameter(\n        \"transl\", nn.Parameter(default_transl, requires_grad=False)\n    )\n\n    # The vertices of the template model\n    self.register_buffer(\n        \"v_template\",\n        to_tensor(to_np(self.flame_model.v_template), dtype=self.dtype),\n    )\n\n    # The shape components\n    shapedirs = self.flame_model.shapedirs\n    # The shape components\n    self.register_buffer(\"shapedirs\", to_tensor(to_np(shapedirs), dtype=self.dtype))\n\n    j_regressor = to_tensor(to_np(self.flame_model.J_regressor), dtype=self.dtype)\n    self.register_buffer(\"J_regressor\", j_regressor)\n\n    # Pose blend shape basis\n    num_pose_basis = self.flame_model.posedirs.shape[-1]\n    posedirs = np.reshape(self.flame_model.posedirs, [-1, num_pose_basis]).T\n    self.register_buffer(\"posedirs\", to_tensor(to_np(posedirs), dtype=self.dtype))\n\n    # indices of parents for each joints\n    parents = to_tensor(to_np(self.flame_model.kintree_table[0])).long()\n    parents[0] = -1\n    self.register_buffer(\"parents\", parents)\n\n    self.register_buffer(\n        \"lbs_weights\", to_tensor(to_np(self.flame_model.weights), dtype=self.dtype)\n    )\n\n    # Static and Dynamic Landmark embeddings for FLAME\n\n    with open(config.static_landmark_embedding_path, \"rb\") as f:\n        static_embeddings = Struct(**pickle.load(f, encoding=\"latin1\"))\n\n    lmk_faces_idx = (static_embeddings.lmk_face_idx).astype(np.int64)\n    self.register_buffer(\n        \"lmk_faces_idx\", torch.tensor(lmk_faces_idx, dtype=torch.long)\n    )\n    lmk_bary_coords = static_embeddings.lmk_b_coords\n    self.register_buffer(\n        \"lmk_bary_coords\", torch.tensor(lmk_bary_coords, dtype=self.dtype)\n    )\n\n    if self.use_face_contour:\n        conture_embeddings = np.load(\n            config.dynamic_landmark_embedding_path,\n            allow_pickle=True,\n            encoding=\"latin1\",\n        )\n        conture_embeddings = conture_embeddings[()]\n        dynamic_lmk_faces_idx = np.array(conture_embeddings[\"lmk_face_idx\"]).astype(\n            np.int64\n        )\n        dynamic_lmk_faces_idx = torch.tensor(\n            dynamic_lmk_faces_idx, dtype=torch.long\n        )\n        self.register_buffer(\"dynamic_lmk_faces_idx\", dynamic_lmk_faces_idx)\n\n        dynamic_lmk_bary_coords = conture_embeddings[\"lmk_b_coords\"]\n        dynamic_lmk_bary_coords = np.array(dynamic_lmk_bary_coords)\n        dynamic_lmk_bary_coords = torch.tensor(\n            dynamic_lmk_bary_coords, dtype=self.dtype\n        )\n        self.register_buffer(\"dynamic_lmk_bary_coords\", dynamic_lmk_bary_coords)\n\n        neck_kin_chain = []\n        curr_idx = torch.tensor(self.NECK_IDX, dtype=torch.long)\n        while curr_idx != -1:\n            neck_kin_chain.append(curr_idx)\n            curr_idx = self.parents[curr_idx]\n        self.register_buffer(\"neck_kin_chain\", torch.stack(neck_kin_chain))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.NECK_IDX","title":"NECK_IDX  <code>instance-attribute</code>","text":"<pre><code>NECK_IDX = 1\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = float32\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.faces","title":"faces  <code>instance-attribute</code>","text":"<pre><code>faces = f\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.flame_model","title":"flame_model  <code>instance-attribute</code>","text":"<pre><code>flame_model = Struct(**(load(f, encoding='latin1')))\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.use_3D_translation","title":"use_3D_translation  <code>instance-attribute</code>","text":"<pre><code>use_3D_translation = use_3D_translation\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.use_face_contour","title":"use_face_contour  <code>instance-attribute</code>","text":"<pre><code>use_face_contour = use_face_contour\n</code></pre>"},{"location":"reference/liblaf/flame_pytorch/upstream/flame/#liblaf.flame_pytorch.upstream.flame.FLAME.forward","title":"forward","text":"<pre><code>forward(\n    shape_params=None,\n    expression_params=None,\n    pose_params=None,\n    neck_pose=None,\n    eye_pose=None,\n    transl=None,\n)\n</code></pre> Input <p>shape_params: N X number of shape parameters expression_params: N X number of expression parameters pose_params: N X number of pose parameters</p> <p>return:     vertices: N X V X 3     landmarks: N X number of landmarks X 3</p> Source code in <code>src/liblaf/flame_pytorch/upstream/flame.py</code> <pre><code>def forward(\n    self,\n    shape_params=None,\n    expression_params=None,\n    pose_params=None,\n    neck_pose=None,\n    eye_pose=None,\n    transl=None,\n):\n    \"\"\"\n    Input:\n        shape_params: N X number of shape parameters\n        expression_params: N X number of expression parameters\n        pose_params: N X number of pose parameters\n    return:\n        vertices: N X V X 3\n        landmarks: N X number of landmarks X 3\n    \"\"\"\n    betas = torch.cat(\n        [shape_params, self.shape_betas, expression_params, self.expression_betas],\n        dim=1,\n    )\n    neck_pose = neck_pose if neck_pose is not None else self.neck_pose\n    eye_pose = eye_pose if eye_pose is not None else self.eye_pose\n    transl = transl if transl is not None else self.transl\n    full_pose = torch.cat(\n        [pose_params[:, :3], neck_pose, pose_params[:, 3:], eye_pose], dim=1\n    )\n    template_vertices = self.v_template.unsqueeze(0).repeat(self.batch_size, 1, 1)\n\n    vertices, _ = lbs(\n        betas,\n        full_pose,\n        template_vertices,\n        self.shapedirs,\n        self.posedirs,\n        self.J_regressor,\n        self.parents,\n        self.lbs_weights,\n    )\n\n    lmk_faces_idx = self.lmk_faces_idx.unsqueeze(dim=0).repeat(self.batch_size, 1)\n    lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(\n        self.batch_size, 1, 1\n    )\n    if self.use_face_contour:\n\n        (\n            dyn_lmk_faces_idx,\n            dyn_lmk_bary_coords,\n        ) = self._find_dynamic_lmk_idx_and_bcoords(\n            vertices,\n            full_pose,\n            self.dynamic_lmk_faces_idx,\n            self.dynamic_lmk_bary_coords,\n            self.neck_kin_chain,\n            dtype=self.dtype,\n        )\n\n        lmk_faces_idx = torch.cat([dyn_lmk_faces_idx, lmk_faces_idx], 1)\n        lmk_bary_coords = torch.cat([dyn_lmk_bary_coords, lmk_bary_coords], 1)\n\n    landmarks = vertices2landmarks(\n        vertices, self.faces_tensor, lmk_faces_idx, lmk_bary_coords\n    )\n\n    if self.use_3D_translation:\n        landmarks += transl.unsqueeze(dim=1)\n        vertices += transl.unsqueeze(dim=1)\n\n    return vertices, landmarks\n</code></pre>"}]}